{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting /MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting /MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist=input_data.read_data_sets(\"/MNIST_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot an image on the screen we will be using the helper function \n",
    "def display_function(digit):\n",
    "    plt.imshow(digit.reshape(28,28),cmap=\"Greys\",interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_digits,trainig_labels=mnist.train.next_batch(1000)\n",
    "test_digits,test_labels=mnist.test.next_batch(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_function(training_digits[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will specify some variables to define our convolution layer \n",
    "height=28\n",
    "width=28\n",
    "channels=1\n",
    "n_inputs=height*width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#designing our first convolution layer \n",
    "conv1_feature_maps=32\n",
    "conv1_kernel_size=3\n",
    "conv1_stride=1\n",
    "conv1_pad='SAME'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#designing  second convolution layer \n",
    "conv2_feature_maps=64\n",
    "conv2_kernel_size=3\n",
    "conv2_stride=2\n",
    "conv2_pad=\"SAME\"   # input to be zero padded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pooling layer will be having same number of feature maps as last convolution layer \n",
    "pool3_feature_maps=conv2_feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After this the output from above will finally goes to your fully connected layer \n",
    "n_fullyconn1=64\n",
    "n_outputs=10   # as we wanted to classify the output category from digits 0 to 9 (means 10 outputs possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(tf.float32,shape=[None,n_inputs],name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CNN, we want the data in the form of 2D\n",
    "x_reshaped=tf.reshape(x,shape=[-1,height,width,channels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=tf.placeholder(tf.int32,shape=[None],name=\"y\")   # This will be a 1D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the convolution layer \n",
    "conv1=tf.layers.conv2d(x_reshaped,filters=conv1_feature_maps,\n",
    "                      kernel_size=conv1_kernel_size,\n",
    "                      strides=conv1_stride,padding=conv1_pad,\n",
    "                      activation=tf.nn.relu,name=\"conv1\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2=tf.layers.conv2d(conv1,filters=conv2_feature_maps,\n",
    "                      kernel_size=conv2_kernel_size,\n",
    "                      strides=conv2_stride,padding=conv2_pad,\n",
    "                      activation=tf.nn.relu,name=\"conv2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(14), Dimension(14), Dimension(64)])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool3=tf.nn.max_pool(conv2,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"VALID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(7), Dimension(7), Dimension(64)])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to feed this into fully connected layer, we need to flatten the result of pooling layer \n",
    "pool3_flat=tf.reshape(pool3,shape=[-1,7*7*pool3_feature_maps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fully connected layers \n",
    "fullyconn1=tf.layers.dense(pool3_flat,n_fullyconn1,activation=tf.nn.relu,name=\"fc1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For softmax layer we need to setup a dense layer \n",
    "logits=tf.layers.dense(fullyconn1,n_outputs,name=\"Output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function for above layer will be \n",
    "xentropy=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=y)\n",
    "\n",
    "# It will takes the dense layer and apply the softmax activation to every neuron in that layer and also uses cross entropy as cost function  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=tf.reduce_mean(xentropy)\n",
    "optimizer=tf.train.AdamOptimizer()\n",
    "training_op=optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct=tf.nn.in_top_k(logits,y,1)   # Checking the act and predicted value (For higher probability)\n",
    "\n",
    "accuracy=tf.reduce_mean(tf.cast(correct,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "init=tf.global_variables_initializer()  \n",
    "saver=tf.train.Saver()                        #Allow us to save our trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Accuracy: 0.98 Test Accuracy: 0.9769\n"
     ]
    }
   ],
   "source": [
    "n_epochs=5\n",
    "batch_size=100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples//batch_size):\n",
    "            \n",
    "            x_batch,y_batch=mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op,feed_dict={x:x_batch,y:y_batch})\n",
    "        \n",
    "        acc_train=accuracy.eval(feed_dict={x:x_batch,y:y_batch})\n",
    "        \n",
    "        acc_test=accuracy.eval(feed_dict={x:mnist.test.images,y:mnist.test.labels})\n",
    "        \n",
    "        \n",
    "        print(epoch,\"Train Accuracy:\",acc_train,\"Test Accuracy:\",acc_test)\n",
    "        \n",
    "        save_path=saver.save(sess,\"./my_minst_model\")\n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
